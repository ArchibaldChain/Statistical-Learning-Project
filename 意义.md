#### 1.RSS

residual sum of squares **残差平方和**
$$
y = e^2_1 + e^2_2 + …… + e^2_n
$$
或者等价地定义为
$$
RSS = (y_1 - \hat\beta_0 - \hat\beta_1x_1)^2 + (y_2 - \hat\beta_0 - \hat\beta_1x_2)^2 + ……+(y_n - \hat\beta_0 - \hat\beta_1x_n)^2
$$


RSS代表了观测到的响应值和用线性模型预测出的响应值之间的差距。

#### 2.TSS

$$
TSS = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

total sum of squares **总平方和**

总平方和测量响应变量Y的总方差，可以认为是在执行回归分析前响应变量中的固有变异性。相对的，RSS测量的是进行回归后任无法解释的变异性。

#### 3.SE

standard error **标准误差**
$$
Var(\hat{\mu}) = SE(\hat{\mu}^2) = \frac{\sigma^2}{n}
$$


标准误差$\hat{\mu}$作为$\mu$的估计能有多准确，标准误差可用于计算置信区间。在一般情况下，$\sigma^2$是未知的，但是可以从数据中估计出来。

标准误差也可以用来对系数进行假设检验 。



#### 4.RSE

residual standard error **残差标准误**
$$
RSE = \sqrt[2]{\frac{RSS}{n-2}}
$$
RSE被认为是对模型失拟的度量。如果用该模型得到的预测值都非常接近真实值，那么RSE会很小。



#### 5.$R^2$统计量

$R^2$统计量是衡量拟合度的另一个标准。$R^2$统计量采取比例的形式，所以它的值总是在0和1之间，与Y的量级无关。
$$
R^2 = \frac{TSS-RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$
$R^2$统计量测量的是Y的变异中能被X解释的部分所占比例。$R^2$统计量接近1说明回归可以解释响应变量的大部分变异。$R^2$统计量接近0说明回归没有解释太多响应变量的变异，这可能是因为线性模型是错误的，也可能因为固有误差项$\sigma^2$较大， 或二者兼有。

$R^2$统计量衡量了X和Y之间的线性关系。协方差$r = Cov(X,Y)$也衡量了X和Y之间的线性关系。在简单回归模型中，$r^2 = R^2$。



#### 6.t统计量

$$
t = \frac{\hat\beta_1 - 0}{SE(\hat\beta_1)}
$$

测量了$\beta_1$偏离0的标准偏差。

t分布有钟型结构，且当n值大于约30时，它很类似于正态分布。



#### 7.P值

**P值**定义等于$|t|$。

P值可以解释如下：一个很小的P值表示，在预测变量(自变量)和相应变量(应变量)之间的真实关系未知的情况下，不太可能完全由于偶然而观察到预测变量和响应变量之间的强相关。因此，如果看到一个很小的P值，就可以推断预测变量和响应变量间存在关联。如果P值足够小，我们便可以拒绝零假设($H_0$:X与Y之间没有关系)声明X和Y之间存在关系。典型的拒绝零假设的临界p值是5%或者1%。当n = 30时，这两个p值相对应的t统计量分别约为2和2.75。

#### F统计量

用在多元统计分析中。
$$
F = \frac{(TSS-RSS)/n}{RSS(n - p - 1)}
$$
当响应变量与与预测变量无关，F统计量应该接近1。另一方面，如果$H_a$为真，那么我们预计F会大于一。如果我们用单独的t统计量及相应的p值确定预测变量与响应变量是否相关，很有可能会错误地得出有相关性的结论。而F统计量就不会有这个问题，因为它会根据预测变量的个数进行调整。









